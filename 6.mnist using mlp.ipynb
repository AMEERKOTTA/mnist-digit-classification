{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ MLP is used to classify the Handwritten Digit from with MNIST Dataset\n",
    "\n",
    "+ Multi-Layer Perceptron is a simple Type of Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Dataset:\n",
    "\n",
    "+ It is a Database of Handwritten Digit that has a training set pf 60,000 examles and a test set of 10,000 examples.\n",
    "\n",
    "+ It is a subset of larger set available from NIST.\n",
    "\n",
    "+ The digits have been size normalized and centered in a fixed size image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ MNIST is a high optimized dataset and it does not contain images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-02b855d8d526>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ one_hot = True: which means that the labels will be presented in a way that to represent a number N, the Nth bit is 1 while the other bits are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported data can be divided as \n",
    "\n",
    "+ Training(mnist.train):\n",
    "    \n",
    "    + Use the given dataset with inputs and related outputs for training of NN.\n",
    "    + This case, an image that you know already is a nine, this set will tell the NN the output expecting as nine.\n",
    "        \n",
    "        + 55,000 datapoints\n",
    "        + mnist.train.images for input\n",
    "        + mnist.train.labels for outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Validation(mnist.validation):\n",
    "    \n",
    "    + Same  as training, but the data used to generate model properties (eg.classification error).\n",
    "    + from this tune parameters like the optinmal no of hidden units.\n",
    "    + or determine the stopping point for back-propogation algorithm.\n",
    "    \n",
    "        + 5,000 datapoints\n",
    "        + mnist.validation.images for inputs\n",
    "        + mnist.validatioin.labels for outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Test Data (mnist.test):\n",
    "\n",
    "    + The model does not have the access to this until testing phase\n",
    "    + it is used to evaluate the perfomance and accuracy of model\n",
    "    + No further optimization beyond this point.\n",
    "    \n",
    "        + 10,000 datapoints\n",
    "        + mnist.test.images for inputs\n",
    "        + mnist.test.labels for outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an Interactive Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two Basic options to run the code in Tensorflow\n",
    "\n",
    "+ Build Graph and Run Session.\n",
    "    \n",
    "     + Do all the set up\n",
    "     + Execute a Session to evaluate the tensors\n",
    "     + run operations\n",
    "       \n",
    "\n",
    "+ Interactive Session\n",
    "\n",
    "     + Create your own code and run on fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward with Interactive Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Placeholders for Inputs(Xs) and Outputs(Ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder X : represents the space allocated input or the images\n",
    "\n",
    "+ Each input has 784 pixels distributed by a 28x28 matrix\n",
    "+ Shape argument defines the tensor size by its dimensions\n",
    "+ 1st Dimension: \"None\" which indicates that Batch size can be of any size.\n",
    "+ 2nd Dimension: 784 indicates the no of pixels on a single flattened mnist image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder Y : represents the output or labels\n",
    "\n",
    "+ 10 possible classes (0,1,2,3,4,5,6,7,8,9)\n",
    "+ shape argument defines the tensor size by its dimensions\n",
    "+ 1st Dimension = None, Batch size can be of any size\n",
    "+ 2nd Dimension = 10, no of targets/ outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dtype for placeholders\n",
    "\n",
    "+ if you are not sure take tf.float32\n",
    "+ Here the limitation is that,the later presented softmax function can only accepts tf.float32 or tf.float64 dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape = [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(?, 784) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights Tensor\n",
    "W = tf.Variable(tf.zeros([784,10], tf.float32))\n",
    "\n",
    "# Biases Tensor\n",
    "b = tf.Variable(tf.zeros([10], tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(784, 10) dtype=float32_ref>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_1:0' shape=(10,) dtype=float32_ref>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Assignment Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Previously, W and b are assigned.\n",
    "+ But, In Tensorflow, they are supposed to initialize (the W and b)\n",
    "+ Use sess.run() to initialize. Since previously started an InteractiveSession in sess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the op initialize_all_variables ising an interactive session\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Weights and Biases to input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The tf.matmul() operation performs a matrix multiplication between X and W and the it sums with b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ibm.box.com/shared/static/88ksiymk1xkb10rgk0jwr3jw814jbfxo.png\" alt=\"HTML5 Icon\" style=\"width:400px;height:350px;\"> \n",
    "<div style=\"text-align:center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Softmax is an Activation Function that is normally used in Classification Problems.\n",
    "\n",
    "+ It generates the probability for the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the one-hot vector for a nine digit label"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0 --> 0  \n",
    "1 --> 0 \n",
    "2 --> 0\n",
    "3 --> 0\n",
    "4 --> 0\n",
    "5 --> 0\n",
    "6 --> 0\n",
    "7 --> 0\n",
    "8 --> 0\n",
    "9 --> 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Example of Probability Distribution:\n",
    "\n",
    "\n",
    "0 -->0.01  \n",
    "1 -->0.02  \n",
    "2 -->0.03  \n",
    "3 -->0.02  \n",
    "4 -->0.12  \n",
    "5 -->0.01  \n",
    "6 -->0.03\n",
    "7 -->0.06\n",
    "8 -->0.1\n",
    "9 -->0.6  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above cell of Probability Distribution, the output can be confirmed as 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W) +b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Logistic Function output is used to classify between two target classes 0/1.\n",
    "\n",
    "+ Softmax Function is a generalized type of Logistic Function.\n",
    "\n",
    "+ That is, Softmax can output a multiclass categorical probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Cost Function is the Function that is used to minimize the difference between the right answers(labels) and estimated output by the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices = [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type of Optimization: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Configure the Optimizer for the Nueral Network\n",
    "\n",
    "+ There are Several optimizer but here using Grdient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'GradientDescent' type=NoOp>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Train using minibatch Gradient Descent\n",
    "\n",
    "+ Batch Gradient Descent is not often used because is computationally expensive.\n",
    "\n",
    "+ In Batch Gradient Descent, the whole dataset is used in Single Time\n",
    "\n",
    "+ So, NN usually use minibatch to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load 50 training examples for each training iteration   \n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final accuracy for the simple ANN model is: 91.1300003528595 % \n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "acc = accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}) * 100\n",
    "print(\"The final accuracy for the simple ANN model is: {} % \".format(acc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Final Accuracy of the Model is 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
